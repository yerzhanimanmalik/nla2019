{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 5: Linear systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Previous lecture\n",
    "- Matrix rank\n",
    "- Skeleton decomposition\n",
    "- Low-rank approximation\n",
    "- Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Today lecture\n",
    "\n",
    "- Linear systems\n",
    "- Inverse matrix\n",
    "- Condition number\n",
    "- Gaussian elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear systems\n",
    "\n",
    "- Linear systems of equations are the basic tool in NLA.\n",
    "\n",
    "- They appear as:\n",
    "\n",
    "    - Linear regression problems\n",
    "    - Discretization of partial differential/integral equations\n",
    "    - Linearizations of nonlinear regression problems\n",
    "    - Optimization (i.e., Gauss-Newton and Newton-Raphson methods, KKT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear equations and matrices\n",
    "- From school we know about linear equations. \n",
    "\n",
    "- A linear system of equations can be written in the form\n",
    "\n",
    "\\begin{align*}\n",
    "    &2 x + 3 y = 5\\quad &\\longrightarrow \\quad &2x + 3 y + 0 z = 5\\\\\n",
    "    &2 x + 3z = 5\\quad &\\longrightarrow\\quad &2 x + 0 y + 3 z = 5\\\\\n",
    "    &x + y = 2\\quad &\\longrightarrow\\quad  & 1 x + 1 y + 0 z = 2\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix form\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "2 & 3 & 0 \\\\\n",
    "2 & 0 & 3 \\\\\n",
    "1 & 1 & 0 \\\\\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "z \n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "5 \\\\\n",
    "5 \\\\\n",
    "2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "or simply\n",
    "\n",
    "$$ A u = f,  $$\n",
    "\n",
    "where $A$ is a $3 \\times 3$ matrix and $f$ is right-hand side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Over/under determined linear systems\n",
    "\n",
    "If the system $Au = f$ has\n",
    "\n",
    "-  more equations than unknowns it is called **overdetermined** system (generically, no solution)\n",
    "\n",
    "- less equations than unknowns it is called **underdetermined** system (solution is non-unique, to make it unique additional assumptions have to be made)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Existence of solutions \n",
    "\n",
    "A solution to the linear system of equations with a **square** matrix $A$\n",
    "\n",
    "$$A u = f$$\n",
    "\n",
    "exists, iff \n",
    "* $\\det A \\ne 0$\n",
    "\n",
    "or\n",
    "\n",
    "* matrix $A$ has full rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scales of linear systems\n",
    "\n",
    "In different applications, the typical size of the linear systems can be different. \n",
    "\n",
    "- Small: $n \\leq 10^4$ (full matrix can be stored in memory, **dense matrix**)\n",
    "- Medium: $n = 10^4 - 10^6$ (typically, **sparse** or **structured** matrix)\n",
    "- Large: $n = 10^8 - 10^9$ (typically **sparse** matrix + parallel computations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear systems can be big\n",
    "- We take a continious problem, discretize it on a mesh with $N$ elements and get a linear system with $N\\times N$ matrix.  \n",
    "- Example of a mesh around A319 aircraft\n",
    "(taken from [GMSH website](http://geuz.org/gmsh/)).  \n",
    "<img src=\"a319_4.png\" width=50%>\n",
    "\n",
    "The main difficulty is that these systems are big: millions or billions of unknowns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear systems can be structured\n",
    "\n",
    "- Storing $N^2$ elements of a matrix is prohibitive even for $N = 100000$.  \n",
    "\n",
    "**Q:** how to work with such matrices?  \n",
    "\n",
    "**A:** fortunately, those matrices are **structured** and require $\\mathcal{O}(N)$ parameters to be stored.  \n",
    "\n",
    "- The most widespread structure are **sparse matrices**: such matrices have only $\\mathcal{O}(N)$ non-zeros!  \n",
    "\n",
    "- Example (one of the famous matrices around for $n = 5$):\n",
    "\n",
    "$$\n",
    "  \\begin{pmatrix}\n",
    "  2 & -1 & 0 & 0 & 0 \\\\\n",
    "  -1 & 2 & -1 & 0 & 0 \\\\\n",
    "  0 & -1 & 2 & -1 & 0 \\\\\n",
    "  0 & 0 &-1& 2 & -1  \\\\\n",
    "  0 & 0 & 0 & -1 & 2 \\\\\n",
    "  \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "- At least you can store such matrices\n",
    "- Also you can multiply such matrix by vector fast\n",
    "- But how to solve linear systems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Main questions about linear systems\n",
    "\n",
    "1. What is the accuracy we get from the solution (due to rounding errors)?\n",
    "2. How we compute the solution? (LU-decomposition, Gaussian elimination)\n",
    "3. What is the complexity of the solution of linear systems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to solve linear systems?\n",
    "**Important**: forget about determinants and the **Cramer rule** (it is  good for $2 \\times 2$ matrices still)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to solve linear systems?\n",
    "\n",
    "The main tool is variable elimination. \n",
    "\n",
    "\\begin{align*}\n",
    "    &2 y + 3 x = 5 \\quad&\\longrightarrow \\quad &y = 5/2 -  3/2 x \\\\\n",
    "    &2 x + 3z = 5 \\quad&\\longrightarrow\\quad &z = 5/3 - 2/3 x\\\\\n",
    "    &x + y = 2 \\quad&\\longrightarrow\\quad  & 5/2 + 5/3 - (3/2 + 2/3) x = 2,\\\\\n",
    "\\end{align*}\n",
    "\n",
    "and that is how you find $x$ (and all previous ones).  \n",
    "\n",
    "This process is called **Gaussian elimination** and is one of the most widely used algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian elimination\n",
    "Gaussian elimination consists of two steps:\n",
    "1. Forward step\n",
    "2. Backward step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Forward step\n",
    "- In the forward step, we eliminate $x_1$:\n",
    "\n",
    "$$\n",
    "   x_1 = f_1 - (a_{12} x_2 + \\ldots + a_{1n} x_n)/a_{11},\n",
    "$$\n",
    "\n",
    "and then substitute this into the equations $2, \\ldots, n$. \n",
    "\n",
    "- Then we eliminate $x_2$ and so on from the second equation. \n",
    "\n",
    "- The important thing is that the **pivots** (that we divide over) are not equal to $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backward step\n",
    "In the backward step:\n",
    "- solve equation for $x_n$\n",
    "- put it into the equation for $x_{n-1}$ and so on, until we \n",
    "compute all $x_i, i=1,\\ldots, n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian elimination and LU decomposition\n",
    "\n",
    "- Gaussian elimination is the computation of one of the most important matrix decompositions: **LU-decomposition**.\n",
    "\n",
    "**Definition**: LU-decomposition of the square matrix $A$ is the representation\n",
    "\n",
    "$$A =  LU,$$\n",
    "\n",
    "where \n",
    "- $L$ is **lower triangular** (elements strictly above the diagonal are zero)\n",
    "- $U$ is **upper triangular** matrix (elements strictly below the diagonal are zero)\n",
    "\n",
    "This factorization is **non-unique**, so it is typical to require that the matrix $L$ has ones on the diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Main goal** of the LU decomposition is to solve linear system, because\n",
    "\n",
    "$$ A^{-1} f = (L U)^{-1} f = U^{-1} L^{-1} f, $$\n",
    "\n",
    "and this reduces to the solution of two linear systems **forward step**\n",
    "\n",
    "$$ L y = f, $$\n",
    "\n",
    "and **backward step**\n",
    "\n",
    "$$ U x = y. $$\n",
    "\n",
    "Does $LU$ decomposition always exist?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Complexity of the Gaussian elimination/LU decomposition\n",
    "\n",
    "- Each elimination step requires $\\mathcal{O}(n^2)$ operations. \n",
    "\n",
    "- Thus, the cost of the naive algorithm is $\\mathcal{O}(n^3)$.  \n",
    "\n",
    "**Think a little bit**: can Strassen algorithm help here? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Block LU-decomposition\n",
    "\n",
    "We can try to compute **block** version of LU-decomposition:\n",
    "\n",
    "\n",
    "$$\\begin{pmatrix} A_{11} & A_{12} \\\\\n",
    "A_{21} & A_{22}\n",
    "\\end{pmatrix} = \\begin{pmatrix} L_{11} & 0 \\\\\n",
    "L_{21} & L_{22}\n",
    "\\end{pmatrix} \\begin{pmatrix} U_{11} & U_{12} \\\\\n",
    "0 & U_{22} \n",
    "\\end{pmatrix} $$\n",
    "\n",
    "- There are two basic operations: compute LU-factorization of half-matrices + matrix-by-matrix product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Existence of the LU-decomposition\n",
    "- The LU-decomposition algorithm does not fail if **we do not divide by zero** at every step of the Gaussian elimination.\n",
    "\n",
    "**Q:** when it is so, for which class of matrices?\n",
    "\n",
    "**A:** it is true for **strictly regular matrices**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Strictly regular matrices and LU decomposition \n",
    "\n",
    "- **Definition.** A matrix $A$ is called *strictly regular*, if all of its **leading principal minors** (i.e, submatrices in the first $k$ rows and $k$ columns) are non-singular. \n",
    "\n",
    "- In this case, there always exists an LU-decomposition. The reverse is also true (check!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Corollary:** If $L$ is unit triangular (ones on the diagonal), then $LU$-decomposition is unique. <br>\n",
    "\n",
    "**Proof:** Indeed, $L_1 U_1 = L_2 U_2$ means $L_2^{-1} L_1 = U_2 U_1^{-1}$. $L_2^{-1} L_1 $ is lower triangular with ones on the diagonal. $U_2 U_1^{-1}$ is upper triangular. Thus, $L_2^{-1} L_1 = U_2 U_1^{-1} = I$ and $L_1 = L_2$, $U_1 = U_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LU for positive definite Hermitian matrices (Cholesky factorization)\n",
    "\n",
    "- Strictly regular matrices have LU-decomposition. \n",
    "\n",
    "- An important **subclass** of strictly regular matrices is the class of **Hermitian positive definite matrices**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "**Definition.** A matrix $A$ is called <font color='red'> positive definite </font> if for any $x: \\Vert x \\Vert \\ne 0$ we have\n",
    "\n",
    "$$\n",
    "(x, Ax) > 0.\n",
    "$$\n",
    "- if this holds for $x \\in \\mathbb{C}^n$, then the matrix $A$ has to be hermitian\n",
    "- if this holds for $x \\in \\mathbb{R}^n$, then the matrix $A$ can be non symmetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **Claim:** A Hermitian positive definite matrix $A$ is strictly regular and has **Cholesky factorization** of the form \n",
    "\n",
    "$$A = RR^*,$$\n",
    "\n",
    "where $R$ is a lower triangular matrix.\n",
    "\n",
    "- Let us try to prove this fact (on the whiteboard).\n",
    "\n",
    "- It is sometimes referred to as \"square root\" of the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing LU-decomposition\n",
    "\n",
    "- In many cases, computing LU-decomposition once is a good idea!\n",
    "\n",
    "- Once the decomposition is found (it costs $\\mathcal{O}(n^3)$ operations), then solving linear systems with $L$ and $U$ costs only $\\mathcal{O}(n^2)$ operations.\n",
    "\n",
    "**Check:**\n",
    "\n",
    "- Solving linear systems with triangular matrices is easy (why?). \n",
    "- How we compute the $L$ and $U$ factors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## When LU fails\n",
    "\n",
    "- What happens, if the matrix is not strictly regular (or the **pivots** in the Gaussian elimination are really small?). \n",
    "\n",
    "- There is classical $2 \\times 2$ example of a matrix with a bad LU decomposition.  \n",
    "\n",
    "- The matrix we look at is  \n",
    "\n",
    "$$\n",
    "    A = \\begin{pmatrix}\n",
    "    \\varepsilon & 1 \\\\\n",
    "    1 & 1 \n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "- If $\\varepsilon$ is sufficiently small, we **might** fail. In contrast the Cholesky factorization is **always stable**.\n",
    "\n",
    "Let us do some demo here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "eps = 1e-18#1.12e-16\n",
    "a = [[eps, 1],[1.0,  1]]\n",
    "a = np.array(a)\n",
    "a0 = a.copy()\n",
    "n = a.shape[0]\n",
    "L = np.zeros((n, n))\n",
    "U = np.zeros((n, n))\n",
    "for k in range(n): #Eliminate one row   \n",
    "    L[k, k] = 1\n",
    "    for i in range(k+1, n):\n",
    "        L[i, k] = a[i, k] / a[k, k]\n",
    "        for j in range(k+1, n):\n",
    "            a[i, j] = a[i, j] - L[i, k] * a[k, j]\n",
    "    for j in range(k, n):\n",
    "        U[k, j] = a[k, j]\n",
    "\n",
    "print('L * U - A:\\n', np.dot(L, U) - a0)\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The concept of pivoting\n",
    "\n",
    "- We can do pivoting, i.e. permute rows and columns to maximize $A_{kk}$ that we divide over.  \n",
    "\n",
    "- The simplest but effective strategy is the **row pivoting**: at each step, select the index that is maximal in modulus, and put it onto the diagonal. \n",
    "\n",
    "- It gives us the decomposition \n",
    "\n",
    "$$A = P L U,$$\n",
    "\n",
    "where $P$ is a **permutation matrix**.\n",
    "\n",
    "\n",
    "**Q.** What makes **row pivoting** good?  \n",
    "\n",
    "**A.** It is made good by the fact that  \n",
    "\n",
    "$$ | L_{ij}|<1, $$\n",
    "\n",
    "but the elements of $U$ can grow, up to $2^n$! (in practice, this is very rarely encountered).\n",
    "\n",
    "- Can you come up with a matrix where the elements of $U$ grow as much as possible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stability of linear systems\n",
    "\n",
    "- There is a fundamental problem of solving linear systems which is independent on the algorithm used.\n",
    "\n",
    "- It occures when elements of a matrix are represented as floating point numbers or there is some measurement noise.\n",
    "\n",
    "Let us illustrate this issue on the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")\n",
    "n = 15\n",
    "a = [[1.0/(i + j + 1) for i in range(n)] for j in range(n)]\n",
    "a = np.array(a)\n",
    "rhs = np.random.randn(n) #Right-hand side\n",
    "x = np.linalg.solve(a, rhs) #This function computes LU-factorization and solves linear system\n",
    "\n",
    "#And check if everything is fine\n",
    "er = np.linalg.norm(a.dot(x) - rhs) / np.linalg.norm(rhs)\n",
    "print(er)\n",
    "plt.plot(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As you see, the error grows with larger $n$, and we have to find out why.  \n",
    "- **Important point** is that it is not a problem of the algorithm: it is a problem of representing the matrix in the memory. \n",
    "- The error occurs in the moment when the matrix elements are evaluated approximately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Questions from the demo\n",
    "\n",
    "- What was the problem in the previous example? \n",
    "\n",
    "- Why the error grows so quickly?  \n",
    "\n",
    "- And here is one of the main concepts of numerical linear algebra: the concept of **condition number** of a matrix.  \n",
    "\n",
    "But before that we have to define the **inverse**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inverse: definition\n",
    "\n",
    "- The inverse of a matrix $A$ is defined as a matrix $X$ denoted by $A^{-1}$ such that  \n",
    "\n",
    "$$ AX = XA = I,  $$\n",
    "\n",
    "where $I$ is the identity matrix (i.e., $I_{ij} = 0$ if $i \\ne j$ and $1$ otherwise).\n",
    "- The computation of the inverse is linked to the solution of linear systems.  Indeed, the $i$-th column of the product gives  \n",
    "\n",
    "$$ A x_i = e_i,$$\n",
    "\n",
    "where $e_i$ is the $i$-th column of the identity matrix. \n",
    "- Thus, we can apply Gaussian elimination to solve this system. Moreover, if there are no divisions by zero in this process (and the pivots do not depend on the right-hand side), then it is possible to solve the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inverse matrix and linear systems\n",
    "If we have computed $A^{-1}$, the solution of linear system  \n",
    "\n",
    "$$Ax = f$$\n",
    "\n",
    "is just $x = A^{-1} f$.  \n",
    "\n",
    "Indeed,  \n",
    "\n",
    "$$ A(A^{-1} f) = (AA^{-1})f = I f = f. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neumann series \n",
    "\n",
    "- To study, why there can be such big errors in a solution (see the example above on the Hilbert matrix) we need an important auxiliary result.  \n",
    "\n",
    "**Neumann series**:  \n",
    "\n",
    "If a matrix $F$ is such that $\\Vert F \\Vert < 1$ holds, then the matrix $(I - F)$ is invertible and\n",
    "\n",
    "$$(I - F)^{-1} = I + F + F^2 + F^3 + \\ldots = \\sum_{k=0}^{\\infty} F^k.$$\n",
    "\n",
    "Note that it is a matrix version of the geometric progression. \n",
    "\n",
    "**Q**: what norm is considered here? What is the \"best possible\" norm here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Proof\n",
    "The proof is constructive. First of all, prove that the series $\\sum_{k=0}^{\\infty} F^k$ converges.  \n",
    "\n",
    "Like in the scalar case, we have  \n",
    "\n",
    "$$ (I - F) \\sum_{k=0}^N F^k = (I - F^{N+1}) \\rightarrow I, \\quad N \\to +\\infty $$\n",
    "\n",
    "Indeed, \n",
    "\n",
    "$$ \\| (I - F^{N+1}) - I\\| = \\|F^{N+1}\\| \\leqslant \\|F\\|^{N+1} \\to 0, \\quad N\\to +\\infty. $$\n",
    "\n",
    "We can also estimate the **norm of the inverse**:\n",
    "\n",
    "$$ \\left\\Vert \\sum_{k=0}^N F^k \\right\\Vert \\leq \\sum_{k=0}^N \\Vert F \\Vert^k \\Vert I \\Vert \\leq \\frac{\\Vert I \\Vert}{1 - \\Vert F \\Vert} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Small perturbation of the inverse\n",
    "\n",
    "- Using this result, we can estimate, how the perturbation of the matrix influences the inverse matrix. \n",
    "- We assume that the perturbation $E$ is small in the sense that $\\Vert A^{-1} E \\Vert < 1$. \n",
    "- Then\n",
    "\n",
    "$$(A + E)^{-1} = \\sum_{k=0}^{\\infty} (-A^{-1} E)^k A^{-1}$$\n",
    "\n",
    "and moreover, \n",
    "\n",
    "$$ \\frac{\\Vert (A + E)^{-1} - A^{-1} \\Vert}{\\Vert A^{-1} \\Vert} \\leq \\frac{\\Vert A^{-1} \\Vert \\Vert E \\Vert \\Vert I \\Vert}{1 - \\Vert A^{-1} E \\Vert}. $$\n",
    "\n",
    "As you see, the norm of the inverse enters the estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Condition number of a linear system\n",
    "Now consider the **perturbed** linear system:\n",
    "\n",
    "$$ (A + \\Delta A) \\widehat{x} = f + \\Delta f. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimates\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\widehat{x} - x &= (A + \\Delta A)^{-1} (f + \\Delta f) - A^{-1} f =\\\\ \n",
    "&= \\left((A + \\Delta A)^{-1} - A^{-1}\\right)f + (A + \\Delta A)^{-1} \\Delta f = \\\\\n",
    "&= \\Big[\\sum_{k=0}^{\\infty} (-A^{-1} \\Delta A)^k\\Big] A^{-1} f + \\Big[\\sum_{k=0}^{\\infty} (A^{-1} \\Delta A)^k \\Big] A^{-1} \\Delta f,\n",
    "\\end{split}\n",
    "$$  \n",
    "\n",
    "therefore\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\Vert \\widehat{x} - x \\Vert}{\\Vert x \\Vert} \\leq \n",
    "&\\frac{\\Vert A \\Vert \\Vert A^{-1} \\Vert}{1 - \\|A^{-1}\\Delta A\\|} \\Big(\\frac{\\Vert\\Delta A\\Vert}{\\Vert A \\Vert} + \\frac{\\Vert \\Delta f \\Vert}{ \\Vert f \\Vert}\\Big) \\leq \\\\\n",
    "\\leq\n",
    "&\\frac{\\Vert A \\Vert \\Vert A^{-1} \\Vert}{1 - \\|A\\|\\|A^{-1}\\|\\frac{\\|\\Delta A\\|}{\\|A\\|}} \\Big(\\frac{\\Vert\\Delta A\\Vert}{\\Vert A \\Vert} + \\frac{\\Vert \\Delta f \\Vert}{ \\Vert f \\Vert}\\Big) \\equiv \\\\\n",
    "\\equiv &\\frac{\\mathrm{cond}(A)}{1 - \\mathrm{cond}(A)\\frac{\\|\\Delta A\\|}{\\|A\\|}} \\Big(\\frac{\\Vert\\Delta A\\Vert}{\\Vert A \\Vert} + \\frac{\\Vert \\Delta f \\Vert}{ \\Vert f \\Vert}\\Big)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "The crucial role is played by the **condition number** $\\mathrm{cond}(A) = \\Vert A \\Vert \\Vert A^{-1} \\Vert$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Condition number \n",
    "\n",
    "- The larger the condition number, the less number of digits we can recover. Note, that the condition number is different for different norms.\n",
    "\n",
    "- Note, that if $\\Delta A = 0$, then\n",
    "\n",
    "$$ \\frac{\\Vert \\widehat{x} - x \\Vert}{\\Vert x \\Vert} \\leq \\mathrm{cond}(A) \\frac{\\|\\Delta f\\|}{\\|f\\|} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The spectral norm of the matrix is equal to the **largest singular value**, and the singular values of the inverse matrix are equal to the inverses of the singular values.  \n",
    "- Thus, the condition number in spectral norm is equal to the ratio of the largest singular value and the smallest singular value.\n",
    "\n",
    "$$ \\mathrm{cond}_2 (A) = \\|A\\|_2 \\|A^{-1}\\|_2 = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hilbert matrix (again)\n",
    "\n",
    "- We can also try to test how tight is the estimate, both with ones in the right-hand side, and with a random vector in the right-hand side. \n",
    "- The results are strickingly different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "n = 1000\n",
    "a = [[1.0/(i + j + 1) for i in range(n)] for j in range(n)]\n",
    "a = np.array(a)\n",
    "rhs = np.ones(n) #Right-hand side\n",
    "f = np.linalg.solve(a, rhs)\n",
    "\n",
    "#And check if everything is fine\n",
    "er = np.linalg.norm(a.dot(f) - rhs) / np.linalg.norm(rhs)\n",
    "cn = np.linalg.cond(a, 2)\n",
    "print('Error:', er, 'Condition number:', cn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And with random right-hand side..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "n = 100\n",
    "a = [[1.0/(i + j + 1) for i in range(n)] for j in range(n)]\n",
    "a = np.array(a)\n",
    "rhs = np.random.randn(n) #Right-hand side\n",
    "f = np.linalg.solve(a, rhs)\n",
    "\n",
    "#And check if everything is fine\n",
    "er = np.linalg.norm(a.dot(f) - rhs) / np.linalg.norm(rhs)\n",
    "cn = np.linalg.cond(a)\n",
    "print('Error:', er, 'Condition number:', cn)\n",
    "\n",
    "u, s, v = np.linalg.svd(a)\n",
    "rhs = np.random.randn(n)\n",
    "plt.plot(u.T.dot(rhs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Can you think about an explanation?**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overdetermined linear systems\n",
    "\n",
    "- Important class of problems are **overdetermined linear systems**, when the number of equations is greater, than the number of unknowns. \n",
    "\n",
    "- The simplest example that you all know, is **linear fitting**, fitting a set of 2D points by a line.\n",
    "\n",
    "- Then, a typical way is to minimize the residual (**least squares**)\n",
    "\n",
    "$$\\Vert A x - b \\Vert_2 \\rightarrow \\min$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overdetermined system and Gram matrix\n",
    "\n",
    "The optimality condition is $0\\equiv \\nabla \\left(\\|Ax-b\\|_2^2\\right)$, where $\\nabla$ denotes gradient. Therefore,\n",
    "\n",
    "$$ 0 \\equiv \\nabla \\left(\\|Ax-b\\|_2^2\\right) = 2(A^*A x - A^*b) = 0. $$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$ \\quad A^* A x = A^* b $$\n",
    "\n",
    "- The matrix $A^* A$ is called **Gram matrix** and the system is called **normal equation**. \n",
    "\n",
    "- This is not a good way to do it, since the condition number of $A^* A$ is a square of condition number of $A$ (check why)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pseudoinverse\n",
    "\n",
    "- Matrix $A^* A$ can be singular in general case. \n",
    "- Therefore, we need to introduce the concept of pseudoinverse matrix  $A^{\\dagger}$ such that solution to the linear least squares problem can formally be written as\n",
    "\n",
    "$$x = A^{\\dagger} b.$$\n",
    "\n",
    "- The matrix $$A^{\\dagger} = \\lim_{\\alpha \\rightarrow 0}(\\alpha I + A^* A)^{-1} A^*$$ is called **Moore-Penrose** pseudoinverse of the matrix $A$.\n",
    "\n",
    "- If matrix $A$ has full column rank, then $A^* A$ is non-singular and we get \n",
    "\n",
    "$$A^{\\dagger} = \\lim_{\\alpha \\rightarrow 0}(\\alpha I + A^* A)^{-1} A^* = (A^* A)^{-1} A^*. $$\n",
    "\n",
    "- If matrix $A$ is squared and non-singular we get standard inverse of $A$:\n",
    "\n",
    "$$A^{\\dagger} = \\lim_{\\alpha \\rightarrow 0}(\\alpha I + A^* A)^{-1} A^* = (A^* A)^{-1} A^* = A^{-1} A^{-*} A^* = A^{-1}$$\n",
    "\n",
    "* If $A$ has linearly dependent columns, then $A^\\dagger b$ gives solution that has minimal Euclidean norm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Compute pseudoinverse via SVD\n",
    "Let $A = U \\Sigma V^*$ be the SVD of $A$. Then,\n",
    "\n",
    "$$A^{\\dagger} = V \\Sigma^{\\dagger} U^*,$$\n",
    "\n",
    "where $\\Sigma^{\\dagger}$ consists of inverses of non-zero singular values of $A$. Indeed,\n",
    "\n",
    "\\begin{align*}\n",
    "A^{\\dagger} &= \\lim_{\\alpha \\rightarrow 0}(\\alpha I + A^* A)^{-1} A^* = \\lim_{\\alpha \\rightarrow 0}( \\alpha VV^* + V \\Sigma^2 V^*)^{-1} V \\Sigma U^* \\\\ & = \\lim_{\\alpha \\rightarrow 0}( V(\\alpha I + \\Sigma^2) V^*)^{-1} V \\Sigma U^* = V \\lim_{\\alpha \\rightarrow 0}(\\alpha I + \\Sigma^2)^{-1} \\Sigma U^* = V \\Sigma^{\\dagger} U^*.\n",
    "\\end{align*}\n",
    "\n",
    "* One can check that $\\Sigma^{\\dagger}$ contains just the inversion of nonzero singular values.\n",
    "* If singular values are small, one can skip inverting them. This will result in a solution which is less sensitive to the noise in the right-hand side.\n",
    "* The condition number for the Euclidean norm is still just the ratio of the largest and the  smallest non-zero singular values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A canonical way to solve linear least squares\n",
    "\n",
    "Is to use the $QR$ decomposition.\n",
    "\n",
    "- Any matrix can be factored into a product \n",
    "\n",
    "$$ A = Q R, $$\n",
    "\n",
    "where $Q$ is unitary, and $R$ is upper triangular (details in the next lectures).\n",
    "\n",
    "- Then, if $A$ has full column rank, then\n",
    "\n",
    "$$ x = A^{\\dagger}b = (A^*A)^{-1}A^*b = ((QR)^*(QR))^{-1}(QR)^*b = (R^*Q^*QR)^{-1}R^*Q^*b = R^{-1}Q^*b.  $$\n",
    "\n",
    "- Thus, finding optimal $x$ is equivalent to solving \n",
    "\n",
    "$$ Rx = Q^* b. $$\n",
    "\n",
    "- Since $R$ is upper triangular, the solving of this linear system costs $\\mathcal{O}(n^2)$. \n",
    "- Also it is more stable, than using the pseudo-inverse matrix directly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Padding into a bigger system\n",
    "\n",
    "- Instead of solving $A^* A x = A^* b$, we introduce a new variable $r = Ax - b$ and then have\n",
    "\n",
    "$$A^* r = 0, \\quad r = Ax - b,$$\n",
    "\n",
    "or in the block form\n",
    "\n",
    "$$ \\begin{pmatrix} 0 & A^* \\\\ A & -I \\end{pmatrix} \\begin{pmatrix} x \\\\ r \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ b \\end{pmatrix}, $$  \n",
    "\n",
    "the total size of the system is $(n + m)$ square, and the condition number is the same as for $A$ \n",
    "- How we define the condition number of a rectangular matrix?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example of LS\n",
    "Consider a two-dimensional example. Suppose we have a linear model \n",
    "\n",
    "$$y = ax + b$$\n",
    "\n",
    "and noisy data $(x_1, y_1), \\dots (x_n, y_n)$. Then the linear system on coefficients will look as follows\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "a x_1 &+ b &= y_1 \\\\\n",
    "&\\vdots \\\\\n",
    "a x_n &+ b &= y_n \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "or in a matrix form\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "x_1 & 1 \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "x_n & 1 \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "a \\\\\n",
    "b\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "y_1 \\\\\n",
    "\\vdots  \\\\\n",
    "y_n \\\\\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "which represents overdetermined system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a_exact = 1.\n",
    "b_exact = 2.\n",
    "\n",
    "n = 10\n",
    "xi = np.arange(n)\n",
    "yi = a_exact * xi + b_exact + 2*np.random.random(n)\n",
    "\n",
    "A = np.array([xi, np.ones(n)])\n",
    "coef = np.linalg.pinv(A).T.dot(yi) # coef is [a, b]\n",
    "\n",
    "plt.plot(xi, yi, 'o', label='$(x_i, y_i)$')\n",
    "plt.plot(xi, coef[0]*xi + coef[1], label='Least Squares')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lacking for structure\n",
    "\n",
    "- A typical 3D-problem requires a $100 \\times 100 \\times 100$ discretization  \n",
    "\n",
    "- This gives a linear system with $10^6$ unknowns, right-hand side takes $8$ megabytes of memory\n",
    "\n",
    "- This matrix has $10^6 \\times 10^6 = 10^{12}$ elements, takes $8$ terabytes of memory.\n",
    "\n",
    "Fortunately, the matrices in real-life are not **dense**, but have certain **structure**:\n",
    "\n",
    "- Sparse matrices\n",
    "- Low-rank matrices\n",
    "- Toeplitz matrices (shift-invariant property)\n",
    "- Sparse in certain bases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "- Linear systems can be solved by Gaussian elimination, complexity is $\\mathcal{O}(n^3)$.\n",
    "- Linear systems can be solved by LU-decomposition, complexity is $\\mathcal{O}(n^3)$ for the decomposition, $\\mathcal{O}(n^2)$ for each solve\n",
    "- Linear least squares can be solved by normal equation (bad)\n",
    "- Linear least squares can be solved by QR-decomposition (good) or by augmentation (not bad)\n",
    "- Without structure, we can solve up to $10^4$ linear systems on a laptop (memory restrictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next lecture\n",
    "- Eigenvectors & eigenvalues \n",
    "- Schur theorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"./styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
